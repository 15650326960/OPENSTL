import os
import numpy as np
import torch
from torch.utils.data import Dataset
import pandas as pd
import glob
from datetime import datetime, timedelta

from scipy.interpolate import LinearNDInterpolator, NearestNDInterpolator

class FusionDataset(Dataset):
    """
    å¤šæºæ•°æ®èåˆé™æ°´ä¸´è¿‘é¢„æŠ¥æ•°æ®é›†
    è¾“å…¥ï¼šè¿‡å»nå¸§çš„å¤šæºæ•°æ®
    è¾“å‡ºï¼šæœªæ¥må¸§çš„é¢„æµ‹æ•°æ®
    """
    def __init__(self, configs, mode='train', year_filter=None):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        Args:
            configs: é…ç½®å‚æ•°
            mode: 'train', 'valid', æˆ– 'test'
            year_filter: å¹´ä»½è¿‡æ»¤å™¨ï¼Œç”¨äºåˆ†æ‰¹åŠ è½½è®­ç»ƒæ•°æ®ï¼Œä¾‹å¦‚[2021, 2022]
        """
        self.configs = configs
        self.mode = mode
        self.year_filter = year_filter

        # åŸºç¡€æ•°æ®è·¯å¾„
        self.base_data_path = configs.get('data_path', '/scratch/jianhao/fusion')
        print(f"ä½¿ç”¨{mode}æ•°æ®åŸºç¡€è·¯å¾„: {self.base_data_path}")

        self.csv_path = configs.get('csv_path', '/home/jianhao/methods/single-source/train_test/csv3')  # è·å–CSVè·¯å¾„
        self.input_length = configs['input_length']  # è¾“å…¥åºåˆ—é•¿åº¦
        self.output_length = configs['output_length']  # è¾“å‡ºåºåˆ—é•¿åº¦

        # åŠ è½½ç›¸åº”çš„CSVæ–‡ä»¶
        csv_file = os.path.join(self.csv_path, f'{mode}.csv')
        if os.path.exists(csv_file):
            self.data_info = pd.read_csv(csv_file, header=None)
            print(f"å·²åŠ è½½{mode}æ•°æ®é›†CSV: {len(self.data_info)}ä¸ªæ ·æœ¬ï¼ŒCSVè·¯å¾„: {csv_file}")
            
            # æ‰“å°CSVæ–‡ä»¶è¯¦ç»†ä¿¡æ¯
            self.print_csv_info(csv_file, mode)
        else:
            raise FileNotFoundError(f"CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_file}")

        # å¦‚æœæ˜¯è®­ç»ƒæ¨¡å¼ä¸”æŒ‡å®šäº†å¹´ä»½è¿‡æ»¤å™¨ï¼Œåˆ™è¿‡æ»¤æ•°æ®
        if mode == 'train' and year_filter is not None:
            self.filter_data_by_years(year_filter)

        # ä¸ºæ¯ä¸€è¡ŒCSVæ•°æ®é¢„å¤„ç†æœ‰æ•ˆç´¢å¼•èŒƒå›´ï¼ˆä¸´æ—¶ç”¨äºæ•°æ®åŠ è½½ï¼‰
        self.valid_ranges = []
        for i, row in self.data_info.iterrows():
            start_time_str = str(row[0])
            end_time_str = str(row[1])

            # è§£ææ—¥æœŸå’Œæ—¶é—´
            start_time = datetime.strptime(start_time_str, "%Y%m%d%H%M")
            end_time = datetime.strptime(end_time_str, "%Y%m%d%H%M")

            # è®¡ç®—å¼€å§‹å’Œç»“æŸç´¢å¼•
            day_start = datetime(start_time.year, start_time.month, start_time.day, 0, 0)
            start_minutes = int((start_time - day_start).total_seconds() / 60)
            end_minutes = int((end_time - day_start).total_seconds() / 60)

            start_idx = start_minutes // 10  # æ¯10åˆ†é’Ÿä¸€ä¸ªæ ·æœ¬
            end_idx = end_minutes // 10

            # ç¡®ä¿æœ‰è¶³å¤Ÿçš„åºåˆ—é•¿åº¦
            total_required_frames = self.input_length + self.output_length
            max_start_idx = end_idx - total_required_frames + 1

            if max_start_idx < start_idx:
                print(f"è­¦å‘Šï¼šè¡Œ{i}çš„æ—¶é—´èŒƒå›´ä¸è¶³ä»¥æå–å®Œæ•´åºåˆ—ï¼Œè·³è¿‡")
                self.valid_ranges.append(None)
            else:
                self.valid_ranges.append((start_idx, max_start_idx, start_time.strftime("%Y%m%d")))

        # è¿‡æ»¤æ‰æ— æ•ˆçš„èŒƒå›´
        valid_indices = [i for i, r in enumerate(self.valid_ranges) if r is not None]
        if len(valid_indices) < len(self.data_info):
            print(f"è­¦å‘Šï¼šæœ‰{len(self.data_info) - len(valid_indices)}è¡Œæ•°æ®å› æ—¶é—´èŒƒå›´ä¸è¶³è¢«è¿‡æ»¤")
            self.data_info = self.data_info.iloc[valid_indices].reset_index(drop=True)
            self.valid_ranges = [r for r in self.valid_ranges if r is not None]

        # é¢„åŠ è½½æ‰€æœ‰æ•°æ®åˆ°å†…å­˜
        self.processed_data = {}  # å­˜å‚¨é¢„å¤„ç†åçš„æ•°æ®
        self.data_masks = {}      # å­˜å‚¨NaNæ©ç 

        # è·å–æ‰€æœ‰éœ€è¦åŠ è½½çš„æ—¥æœŸ
        unique_dates = set([date_str for _, _, date_str in self.valid_ranges])
        print(f"å¼€å§‹é¢„åŠ è½½{mode}æ•°æ®é›†ï¼Œå…±{len(unique_dates)}ä¸ªæ—¥æœŸ...")
        
        for date_str in unique_dates:
            # æ ¹æ®æ—¥æœŸç¡®å®šå¹´ä»½å’Œå¯¹åº”çš„æ•°æ®è·¯å¾„
            year = date_str[:4]
            data_file = os.path.join(self.base_data_path, year, f"fusion_{date_str}.npy")
            try:
                # åŠ è½½åŸå§‹æ•°æ®
                daily_data = np.load(data_file)  # å½¢çŠ¶åº”ä¸º (144, 4, 500, 900)

                # åˆ›å»ºNaNæ©ç ï¼ˆTrueè¡¨ç¤ºæ˜¯NaNï¼‰


                # ä½¿ç”¨æ—¶ç©ºæ’å€¼å¡«å……NaN
                filled_data = self.interpolate_nans(daily_data.copy())

                # è½¬æ¢ä¸ºtorchå¼ é‡
                filled_data = torch.from_numpy(filled_data).float()
                num_weight = (filled_data > -100).float()
                filled = filled_data * num_weight
                filled_mask = (filled == 0.0).bool()

                # å­˜å‚¨é¢„å¤„ç†åçš„æ•°æ®å’Œæ©ç 
                self.processed_data[date_str] = filled
                self.data_masks[date_str] = filled_mask
                print(f"å·²åŠ è½½å¹¶é¢„å¤„ç† {date_str} æ•°æ®ï¼Œå½¢çŠ¶: {filled_data.shape}")
            except Exception as e:
                print(f"åŠ è½½æ–‡ä»¶ {data_file} å¤±è´¥: {e}")
                # åˆ›å»ºç©ºæ•°æ®å’Œæ©ç 
                self.processed_data[date_str] = torch.zeros((144, 4, 500, 900), dtype=torch.float32)
                self.data_masks[date_str] = torch.ones((144, 4, 500, 900), dtype=torch.bool)  # å…¨éƒ¨æ ‡è®°ä¸ºNaN
        
        print(f"{mode}æ•°æ®é›†é¢„åŠ è½½å®Œæˆï¼Œå…±{len(self.processed_data)}ä¸ªæ—¥æœŸçš„æ•°æ®")

        # åˆ†ææ•°æ®è¿ç»­æ€§å¹¶æ„å»ºæ»‘åŠ¨çª—å£æ ·æœ¬
        self.analyze_data_continuity()
        self.build_continuous_timeline()
        self.generate_sliding_samples()

    def print_csv_info(self, csv_file, mode):
        """
        æ‰“å°CSVæ–‡ä»¶è¯¦ç»†ä¿¡æ¯åˆ°ç»ˆç«¯
        Args:
            csv_file: CSVæ–‡ä»¶è·¯å¾„
            mode: æ•°æ®é›†æ¨¡å¼ ('train', 'valid', 'test')
        """
        try:
            # è·å–æ–‡ä»¶ä¿¡æ¯
            file_size = os.path.getsize(csv_file) / 1024  # KB
            file_stat = os.stat(csv_file)
            last_modified = datetime.fromtimestamp(file_stat.st_mtime)
            
            # è¯»å–CSVå†…å®¹è¿›è¡Œè¯¦ç»†åˆ†æ
            df = pd.read_csv(csv_file, header=None)
            total_samples = len(df)
            
            # åˆ†ææ—¶é—´èŒƒå›´
            if len(df) > 0:
                start_times = df[0].astype(str)
                end_times = df[1].astype(str)
                
                # è§£ææ—¶é—´èŒƒå›´
                start_dates = [datetime.strptime(str(t), "%Y%m%d%H%M") for t in start_times]
                end_dates = [datetime.strptime(str(t), "%Y%m%d%H%M") for t in end_times]
                
                min_date = min(start_dates)
                max_date = max(end_dates)
                date_range = f"{min_date.strftime('%Y-%m-%d %H:%M')} åˆ° {max_date.strftime('%Y-%m-%d %H:%M')}"
                
                # ç»Ÿè®¡å¹´ä»½åˆ†å¸ƒ
                years = [d.year for d in start_dates]
                year_counts = pd.Series(years).value_counts().sort_index()
                year_distribution = ", ".join([f"{year}: {count}" for year, count in year_counts.items()])
            else:
                date_range = "æ— æ•°æ®"
                year_distribution = "æ— æ•°æ®"
            
            # æ‰“å°è¯¦ç»†ä¿¡æ¯åˆ°ç»ˆç«¯
            print("=" * 80)
            print(f"ğŸ“Š {mode.upper()} æ•°æ®é›†CSVæ–‡ä»¶è¯¦ç»†ä¿¡æ¯")
            print("=" * 80)
            print(f"ğŸ“ æ–‡ä»¶è·¯å¾„: {csv_file}")
            print(f"ğŸ“ æ–‡ä»¶å¤§å°: {file_size:.2f} KB")
            print(f"ğŸ“… æœ€åä¿®æ”¹: {last_modified.strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"ğŸ“Š æ ·æœ¬æ€»æ•°: {total_samples:,}")
            print(f"ğŸ“… æ—¶é—´èŒƒå›´: {date_range}")
            print(f"ğŸ“ˆ å¹´ä»½åˆ†å¸ƒ: {year_distribution}")
            print("=" * 80)
            
            # æ‰“å°CSVæ–‡ä»¶å†…å®¹
            print(f"ğŸ“„ {mode.upper()} æ•°æ®é›†CSVæ–‡ä»¶å†…å®¹:")
            print("-" * 80)
            print("æ‰€æœ‰æ•°æ®:")
            print(df.to_string(index=False))
            print("-" * 80)
            
        except Exception as e:
            print(f"âš ï¸  æ‰“å°CSVæ–‡ä»¶ä¿¡æ¯æ—¶å‡ºé”™: {e}")

    def filter_data_by_years(self, year_filter):
        """
        æ ¹æ®å¹´ä»½è¿‡æ»¤æ•°æ®
        Args:
            year_filter: å¹´ä»½åˆ—è¡¨ï¼Œä¾‹å¦‚[2021, 2022]
        """
        print(f"æ­£åœ¨è¿‡æ»¤æ•°æ®ï¼Œåªä¿ç•™å¹´ä»½: {year_filter}")
        original_count = len(self.data_info)

        # è¿‡æ»¤æ•°æ®
        filtered_indices = []
        for i, row in self.data_info.iterrows():
            start_time_str = str(row[0])
            year = int(start_time_str[:4])
            if year in year_filter:
                filtered_indices.append(i)

        self.data_info = self.data_info.iloc[filtered_indices].reset_index(drop=True)
        print(f"è¿‡æ»¤å®Œæˆï¼šåŸå§‹{original_count}æ¡æ•°æ®ï¼Œè¿‡æ»¤å{len(self.data_info)}æ¡æ•°æ®")

    def clear_data(self):
        """
        æ¸…ç©ºå·²åŠ è½½çš„æ•°æ®ï¼Œé‡Šæ”¾å†…å­˜
        """
        print("æ­£åœ¨æ¸…ç©ºæ•°æ®é›†å†…å­˜...")
        # æ˜¾å¼åˆ é™¤æ•°æ®å¼•ç”¨
        for key in list(self.processed_data.keys()):
            del self.processed_data[key]
        for key in list(self.data_masks.keys()):
            del self.data_masks[key]

        self.processed_data.clear()
        self.data_masks.clear()

        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        import gc
        gc.collect()
        print("æ•°æ®é›†å†…å­˜å·²æ¸…ç©º")

    def reload_data_with_years(self, year_filter):
        """
        é‡æ–°åŠ è½½æŒ‡å®šå¹´ä»½çš„æ•°æ®
        Args:
            year_filter: å¹´ä»½åˆ—è¡¨ï¼Œä¾‹å¦‚[2023, 2024, 2025]
        """
        print(f"é‡æ–°åŠ è½½å¹´ä»½æ•°æ®: {year_filter}")

        # æ¸…ç©ºå½“å‰æ•°æ®
        self.clear_data()

        # é‡æ–°åŠ è½½CSVå¹¶è¿‡æ»¤
        csv_file = os.path.join(self.csv_path, f'{self.mode}.csv')
        self.data_info = pd.read_csv(csv_file, header=None)
        
        # æ‰“å°é‡æ–°åŠ è½½çš„CSVæ–‡ä»¶ä¿¡æ¯
        print(f"ğŸ”„ é‡æ–°åŠ è½½æ•°æ®ï¼Œå¹´ä»½è¿‡æ»¤å™¨: {year_filter}")
        self.print_csv_info(csv_file, f"{self.mode}_reloaded")
        
        self.filter_data_by_years(year_filter)
        for i, row in self.data_info.iterrows():
            start_time_str = str(row[0])
            end_time_str = str(row[1])

            # è§£ææ—¥æœŸå’Œæ—¶é—´
            start_time = datetime.strptime(start_time_str, "%Y%m%d%H%M")
            end_time = datetime.strptime(end_time_str, "%Y%m%d%H%M")

            # è®¡ç®—å¼€å§‹å’Œç»“æŸç´¢å¼•
            day_start = datetime(start_time.year, start_time.month, start_time.day, 0, 0)
            start_minutes = int((start_time - day_start).total_seconds() / 60)
            end_minutes = int((end_time - day_start).total_seconds() / 60)

            start_idx = start_minutes // 10  # æ¯10åˆ†é’Ÿä¸€ä¸ªæ ·æœ¬
            end_idx = end_minutes // 10

            # ç¡®ä¿æœ‰è¶³å¤Ÿçš„åºåˆ—é•¿åº¦
            total_required_frames = self.input_length + self.output_length
            max_start_idx = end_idx - total_required_frames + 1

            if max_start_idx < start_idx:
                print(f"è­¦å‘Šï¼šè¡Œ{i}çš„æ—¶é—´èŒƒå›´ä¸è¶³ä»¥æå–å®Œæ•´åºåˆ—ï¼Œè·³è¿‡")
                self.valid_ranges.append(None)
            else:
                self.valid_ranges.append((start_idx, max_start_idx, start_time.strftime("%Y%m%d")))

        # è¿‡æ»¤æ‰æ— æ•ˆçš„èŒƒå›´
        valid_indices = [i for i, r in enumerate(self.valid_ranges) if r is not None]
        if len(valid_indices) < len(self.data_info):
            print(f"è­¦å‘Šï¼šæœ‰{len(self.data_info) - len(valid_indices)}è¡Œæ•°æ®å› æ—¶é—´èŒƒå›´ä¸è¶³è¢«è¿‡æ»¤")
            self.data_info = self.data_info.iloc[valid_indices].reset_index(drop=True)
            self.valid_ranges = [r for r in self.valid_ranges if r is not None]

        # é‡æ–°åŠ è½½æ•°æ®
        self.processed_data = {}
        self.data_masks = {}

        # è·å–æ‰€æœ‰éœ€è¦åŠ è½½çš„æ—¥æœŸ
        unique_dates = set([date_str for _, _, date_str in self.valid_ranges])
        print(f"å¼€å§‹é‡æ–°åŠ è½½æ•°æ®ï¼Œå…±{len(unique_dates)}ä¸ªæ—¥æœŸ...")

        for date_str in unique_dates:
            # æ ¹æ®æ—¥æœŸç¡®å®šå¹´ä»½å’Œå¯¹åº”çš„æ•°æ®è·¯å¾„
            year = date_str[:4]
            data_file = os.path.join(self.base_data_path, year, f"fusion_{date_str}.npy")
            try:
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if not os.path.exists(data_file):
                    print(f"æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
                    raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")

                # è·å–æ–‡ä»¶å¤§å°ä¿¡æ¯
                file_size = os.path.getsize(data_file) / (1024**3)  # GB
                print(f"æ­£åœ¨åŠ è½½ {date_str}ï¼Œæ–‡ä»¶å¤§å°: {file_size:.2f} GB")

                # å°è¯•å®‰å…¨åŠ è½½æ•°æ®
                try:
                    # é¦–å…ˆå°è¯•å†…å­˜æ˜ å°„æ¨¡å¼æ£€æŸ¥æ–‡ä»¶
                    data_mmap = np.load(data_file, mmap_mode='r')
                    expected_shape = (144, 4, 500, 900)

                    if data_mmap.shape != expected_shape:
                        print(f"è­¦å‘Š: {date_str} æ•°æ®å½¢çŠ¶å¼‚å¸¸: {data_mmap.shape}, æœŸæœ›: {expected_shape}")
                        # å¦‚æœå½¢çŠ¶ä¸å¯¹ï¼Œè·³è¿‡è¿™ä¸ªæ–‡ä»¶
                        raise ValueError(f"æ•°æ®å½¢çŠ¶å¼‚å¸¸: {data_mmap.shape}")

                    # å¼ºåˆ¶è½¬æ¢ä¸ºfloat32ä»¥èŠ‚çœå†…å­˜
                    daily_data = np.array(data_mmap, dtype=np.float32)
                    del data_mmap  # é‡Šæ”¾å†…å­˜æ˜ å°„

                except Exception as load_error:
                    print(f"å†…å­˜æ˜ å°„åŠ è½½å¤±è´¥ï¼Œå°è¯•ç›´æ¥åŠ è½½: {load_error}")
                    # å¦‚æœå†…å­˜æ˜ å°„å¤±è´¥ï¼Œå°è¯•ç›´æ¥åŠ è½½
                    daily_data = np.load(data_file).astype(np.float32)

                # ä½¿ç”¨æ—¶ç©ºæ’å€¼å¡«å……NaN
                filled_data = self.interpolate_nans(daily_data.copy())
                del daily_data  # é‡Šæ”¾åŸå§‹æ•°æ®å†…å­˜

                # è½¬æ¢ä¸ºtorchå¼ é‡
                filled_data = torch.from_numpy(filled_data).float()
                num_weight = (filled_data > -100).float()
                filled = filled_data * num_weight
                filled_mask = (filled == 0.0).bool()

                # å­˜å‚¨é¢„å¤„ç†åçš„æ•°æ®å’Œæ©ç 
                self.processed_data[date_str] = filled
                self.data_masks[date_str] = filled_mask
                print(f"å·²é‡æ–°åŠ è½½å¹¶é¢„å¤„ç† {date_str} æ•°æ®ï¼Œå½¢çŠ¶: {filled_data.shape}")

            except Exception as e:
                print(f"åŠ è½½æ–‡ä»¶ {data_file} å¤±è´¥: {e}")
                print(f"é”™è¯¯è¯¦æƒ…: {type(e).__name__}: {str(e)}")
                # åˆ›å»ºç©ºæ•°æ®å’Œæ©ç 
                try:
                    self.processed_data[date_str] = torch.zeros((144, 4, 500, 900), dtype=torch.float32)
                    self.data_masks[date_str] = torch.ones((144, 4, 500, 900), dtype=torch.bool)  # å…¨éƒ¨æ ‡è®°ä¸ºNaN
                except Exception as fallback_error:
                    print(f"åˆ›å»ºç©ºæ•°æ®ä¹Ÿå¤±è´¥: {fallback_error}")
                    # å¦‚æœè¿åˆ›å»ºç©ºæ•°æ®éƒ½å¤±è´¥ï¼Œè¯´æ˜å†…å­˜çœŸçš„ä¸è¶³äº†
                    import gc
                    gc.collect()
                    continue

        print(f"é‡æ–°åŠ è½½å®Œæˆï¼Œå…±{len(self.processed_data)}ä¸ªæ—¥æœŸçš„æ•°æ®")

        # é‡æ–°åˆ†ææ•°æ®è¿ç»­æ€§å¹¶ç”Ÿæˆæ»‘åŠ¨çª—å£æ ·æœ¬
        self.analyze_data_continuity()
        self.build_continuous_timeline()
        self.generate_sliding_samples()

    def interpolate_nans(self, data):
        """
        ä½¿ç”¨æ—¶ç©ºæ’å€¼å¡«å……NaNå€¼
        Args:
            data: åŸå§‹æ•°æ®ï¼Œå½¢çŠ¶ä¸º (T, C, H, W)
        Returns:
            filled_data: å¡«å……åçš„æ•°æ®
        """
        # æ—¶é—´ç»´åº¦æ’å€¼
        for c in range(data.shape[1]):
            for h in range(data.shape[2]):
                for w in range(data.shape[3]):
                    series = data[:, c, h, w]
                    if np.any(np.isnan(series)):
                        valid_indices = np.where(~np.isnan(series))[0]
                        if len(valid_indices) > 0:  # å¦‚æœæœ‰æœ‰æ•ˆå€¼
                            nan_indices = np.where(np.isnan(series))[0]
                            # ä½¿ç”¨æœ‰æ•ˆå€¼è¿›è¡Œæ’å€¼
                            series[nan_indices] = np.interp(
                                nan_indices, valid_indices, series[valid_indices],
                                left=0, right=0  # è¾¹ç•Œå¤–ä½¿ç”¨0
                            )
        
        # ç©ºé—´ç»´åº¦æ’å€¼ - å¯¹äºæ—¶é—´æ’å€¼åä»ç„¶æœ‰NaNçš„ä½ç½®
        for t in range(data.shape[0]):
            for c in range(data.shape[1]):
                frame = data[t, c]
                if np.any(np.isnan(frame)):
                    # è·å–éNaNä½ç½®çš„åæ ‡å’Œå€¼
                    y_indices, x_indices = np.where(~np.isnan(frame))
                    if len(y_indices) > 0:  # å¦‚æœæœ‰æœ‰æ•ˆå€¼
                        points = np.column_stack((y_indices, x_indices))
                        values = frame[~np.isnan(frame)]
                        
                        # è·å–NaNä½ç½®çš„åæ ‡
                        nan_y_indices, nan_x_indices = np.where(np.isnan(frame))
                        nan_points = np.column_stack((nan_y_indices, nan_x_indices))
                        
                        # ä½¿ç”¨æœ€è¿‘é‚»æ’å€¼
                        if len(points) > 3:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç‚¹è¿›è¡Œæ’å€¼
                            try:
                                interpolator = NearestNDInterpolator(points, values)
                                frame[nan_y_indices, nan_x_indices] = interpolator(nan_points)
                            except Exception as e:
                                print(f"ç©ºé—´æ’å€¼å¤±è´¥: {e}")
                                frame[np.isnan(frame)] = 0.0  # å¦‚æœæ’å€¼å¤±è´¥ï¼Œä½¿ç”¨0å¡«å……
                        else:
                            frame[np.isnan(frame)] = 0.0  # å¦‚æœæœ‰æ•ˆç‚¹å¤ªå°‘ï¼Œä½¿ç”¨0å¡«å……
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰NaNï¼Œå¦‚æœæœ‰åˆ™å¡«å……ä¸º0
        if np.any(np.isnan(data)):
            data = np.nan_to_num(data, nan=0.0)
            
        return data

    def analyze_data_continuity(self):
        """åˆ†ææ•°æ®çš„æ—¶é—´è¿ç»­æ€§"""
        sorted_dates = sorted(self.processed_data.keys())

        for i, date_str in enumerate(sorted_dates):
            daily_data = self.processed_data[date_str]
            daily_mask = self.data_masks[date_str]

            # åˆ†æå½“å¤©çš„æœ‰æ•ˆæ—¶é—´ç‚¹
            valid_timepoints = []
            for t in range(daily_data.shape[0]):
                if not daily_mask[t].all():
                    valid_timepoints.append(t)

            # åªåœ¨æ•°æ®å¼‚å¸¸æ—¶è¾“å‡ºè­¦å‘Š
            if len(valid_timepoints) < 100:
                print(f"è­¦å‘Š: {date_str} åªæœ‰ {len(valid_timepoints)} ä¸ªæœ‰æ•ˆæ—¶é—´ç‚¹")

    def check_day_continuity(self, date1, date2):
        """æ£€æŸ¥ä¸¤å¤©ä¹‹é—´æ˜¯å¦æ—¶é—´è¿ç»­"""
        # è§£ææ—¥æœŸ
        d1 = datetime.strptime(date1, "%Y%m%d")
        d2 = datetime.strptime(date2, "%Y%m%d")

        # æ£€æŸ¥æ˜¯å¦æ˜¯è¿ç»­çš„ä¸¤å¤©
        if d2 - d1 != timedelta(days=1):
            return False

        # æ£€æŸ¥ç¬¬ä¸€å¤©çš„æœ€åæ—¶é—´ç‚¹å’Œç¬¬äºŒå¤©çš„ç¬¬ä¸€æ—¶é—´ç‚¹æ˜¯å¦è¿ç»­
        data1 = self.processed_data[date1]
        data2 = self.processed_data[date2]
        mask1 = self.data_masks[date1]
        mask2 = self.data_masks[date2]

        # æ‰¾åˆ°ç¬¬ä¸€å¤©æœ€åçš„æœ‰æ•ˆæ—¶é—´ç‚¹
        last_valid_t1 = None
        for t in range(data1.shape[0] - 1, -1, -1):
            if not mask1[t].all():
                last_valid_t1 = t
                break

        # æ‰¾åˆ°ç¬¬äºŒå¤©ç¬¬ä¸€ä¸ªæœ‰æ•ˆæ—¶é—´ç‚¹
        first_valid_t2 = None
        for t in range(data2.shape[0]):
            if not mask2[t].all():
                first_valid_t2 = t
                break

        # æ£€æŸ¥æ—¶é—´è¿ç»­æ€§
        if last_valid_t1 is not None and first_valid_t2 is not None:
            return self.is_time_continuous(last_valid_t1, first_valid_t2)

        return False

    def is_time_continuous(self, t1_end, t2_start):
        """æ£€æŸ¥ä¸¤ä¸ªæ—¶é—´ç‚¹æ˜¯å¦è¿ç»­ï¼ˆè€ƒè™‘è·¨å¤©ï¼‰"""
        # å¦‚æœç¬¬ä¸€å¤©ç»“æŸäº23:50(t=143)ï¼Œç¬¬äºŒå¤©å¼€å§‹äº00:00(t=0)ï¼Œåˆ™è¿ç»­
        if t1_end == 143 and t2_start == 0:
            return True
        # å¯ä»¥æ·»åŠ å…¶ä»–è¿ç»­æ€§åˆ¤æ–­é€»è¾‘
        return False

    def build_continuous_timeline(self):
        """æ„å»ºå…¨å±€è¿ç»­çš„æ—¶é—´è½´"""
        self.global_timeline = []

        sorted_dates = sorted(self.processed_data.keys())
        current_segment = []

        for i, date_str in enumerate(sorted_dates):
            daily_data = self.processed_data[date_str]
            daily_mask = self.data_masks[date_str]

            day_valid_points = []
            for t in range(daily_data.shape[0]):
                if not daily_mask[t].all():
                    day_valid_points.append({
                        'date': date_str,
                        'time_idx': t,
                        'global_time': f"{date_str}_{t:03d}",
                        'actual_time': f"{t*10//60:02d}:{t*10%60:02d}"
                    })

            if not current_segment:
                current_segment = day_valid_points
            else:
                prev_date = sorted_dates[i-1]
                if self.check_day_continuity(prev_date, date_str):
                    current_segment.extend(day_valid_points)
                else:
                    if len(current_segment) >= 24:
                        self.global_timeline.append(current_segment)
                    current_segment = day_valid_points

        if len(current_segment) >= 24:
            self.global_timeline.append(current_segment)

        # åªè¾“å‡ºæ±‡æ€»ä¿¡æ¯
        total_timepoints = sum(len(segment) for segment in self.global_timeline)
        print(f"æ•°æ®åˆ†æå®Œæˆ: {len(self.global_timeline)} ä¸ªè¿ç»­æ—¶é—´æ®µ, å…± {total_timepoints} ä¸ªæœ‰æ•ˆæ—¶é—´ç‚¹")

    def generate_sliding_samples(self):
        """ç”Ÿæˆæ ·æœ¬ï¼šè®­ç»ƒæ¨¡å¼ä½¿ç”¨æ»‘åŠ¨çª—å£ï¼Œæµ‹è¯•æ¨¡å¼ä½¿ç”¨éé‡å çª—å£"""
        self.sliding_samples = []

        if self.mode == 'test':
            # æµ‹è¯•æ¨¡å¼ï¼šç”Ÿæˆè¿ç»­é¢„æµ‹æ ·æœ¬
            for segment_idx, segment in enumerate(self.global_timeline):
                segment_samples = []
                
                # è¿ç»­é¢„æµ‹é‡‡æ ·ï¼šæ¯æ¬¡è·³è¿‡12ä¸ªæ—¶é—´ç‚¹ï¼ˆoutput_lengthï¼‰ï¼Œå®ç°è¿ç»­é¢„æµ‹
                # æ ·æœ¬0: 00:00-01:50 â†’ 02:00-03:50
                # æ ·æœ¬1: 02:00-03:50 â†’ 04:00-05:50
                for start_pos in range(0, len(segment) - 24 + 1, 12):  # step=12
                    input_points = segment[start_pos:start_pos + 12]
                    target_points = segment[start_pos + 12:start_pos + 24]

                    sample = {
                        'segment_idx': segment_idx,
                        'start_pos': start_pos,
                        'input_points': input_points,
                        'target_points': target_points,
                    }

                    segment_samples.append(sample)

                self.sliding_samples.extend(segment_samples)
            
            print(f"è¿ç»­é¢„æµ‹æ ·æœ¬ç”Ÿæˆå®Œæˆ: å…± {len(self.sliding_samples)} ä¸ªæµ‹è¯•æ ·æœ¬")
        else:
            # è®­ç»ƒå’ŒéªŒè¯æ¨¡å¼ï¼šä½¿ç”¨æ»‘åŠ¨çª—å£
            for segment_idx, segment in enumerate(self.global_timeline):
                segment_samples = []

                for start_pos in range(len(segment) - 24 + 1):
                    input_points = segment[start_pos:start_pos + 12]
                    target_points = segment[start_pos + 12:start_pos + 24]

                    sample = {
                        'segment_idx': segment_idx,
                        'start_pos': start_pos,
                        'input_points': input_points,
                        'target_points': target_points,
                    }

                    segment_samples.append(sample)

                self.sliding_samples.extend(segment_samples)

            print(f"æ»‘åŠ¨çª—å£æ ·æœ¬ç”Ÿæˆå®Œæˆ: å…± {len(self.sliding_samples)} ä¸ª{self.mode}æ ·æœ¬")

    def __len__(self):
        """è¿”å›æ•°æ®é›†å¤§å°"""
        return len(self.sliding_samples)
    
    def __getitem__(self, index):
        """è·å–æŒ‡å®šç´¢å¼•çš„æ ·æœ¬"""
        sample = self.sliding_samples[index]

        # æ„å»ºè¾“å…¥å¼ é‡
        input_tensors = []
        input_masks = []

        for point in sample['input_points']:
            date_str = point['date']
            time_idx = point['time_idx']

            daily_data = self.processed_data[date_str]
            daily_mask = self.data_masks[date_str]

            input_tensors.append(daily_data[time_idx:time_idx+1, 0:3])  # [1, 1, 500, 900]
            input_masks.append(daily_mask[time_idx:time_idx+1, 1:2])

        # æ„å»ºç›®æ ‡å¼ é‡
        target_tensors = []
        target_masks = []

        for point in sample['target_points']:
            date_str = point['date']
            time_idx = point['time_idx']

            daily_data = self.processed_data[date_str]
            daily_mask = self.data_masks[date_str]

            target_tensors.append(daily_data[time_idx:time_idx+1, 1:2])
            target_masks.append(daily_mask[time_idx:time_idx+1, 1:2])

        # æ‹¼æ¥å¼ é‡
        input_tensor = torch.cat(input_tensors, dim=0)    # [12, 1, 500, 900]
        input_mask = torch.cat(input_masks, dim=0)
        target_tensor = torch.cat(target_tensors, dim=0)  # [12, 1, 500, 900]
        target_mask = torch.cat(target_masks, dim=0)

        # å¦‚æœæ˜¯æµ‹è¯•æ¨¡å¼ï¼ŒåŒæ—¶è¿”å›æ—¶é—´ä¿¡æ¯
        if self.mode == 'test':
            # æå–æ—¶é—´ä¿¡æ¯
            input_times = [f"{point['date']}_{point['time_idx']:03d}" for point in sample['input_points']]
            target_times = [f"{point['date']}_{point['time_idx']:03d}" for point in sample['target_points']]
            
            return input_tensor, input_mask, target_tensor, target_mask, input_times, target_times
        else:
            return input_tensor, input_mask, target_tensor, target_mask


class FusionDatasetHindcast(Dataset):
    """ç”¨äºå›æº¯æµ‹è¯•çš„æ•°æ®é›†"""
    
    def __init__(self, configs):
        """åˆå§‹åŒ–å›æº¯æµ‹è¯•æ•°æ®é›†"""
        self.configs = configs
        self.data_path = configs['data_path']
        self.input_length = configs['input_length']
        self.csv_path = configs.get('csv_path', self.data_path)  # è·å–CSVè·¯å¾„ï¼Œé»˜è®¤ä¸æ•°æ®è·¯å¾„ç›¸åŒ
        
        # å¦‚æœæä¾›äº†ç‰¹å®šæ—¥æœŸï¼Œåˆ™ä½¿ç”¨è¯¥æ—¥æœŸ
        if 'test_date' in configs:
            self.test_date = configs['test_date']
        else:
            # å°è¯•ä»è·¯å¾„ä¸­æå–æ—¥æœŸ
            test_date = os.path.basename(self.data_path)
            if not test_date.isdigit() or len(test_date) != 8:
                # å¦‚æœç›®å½•åä¸æ˜¯æ—¥æœŸæ ¼å¼ï¼Œåˆ™å°è¯•ä»è·¯å¾„ä¸­æå–
                parts = self.data_path.split('/')
                for part in parts:
                    if part.isdigit() and len(part) == 8:
                        test_date = part
                        break
            self.test_date = test_date
        
        # æ„å»ºæ•°æ®æ–‡ä»¶è·¯å¾„
        self.data_file = os.path.join(self.data_path, f"fusion_{self.test_date}.npy")
        
        # åŠ è½½æ•°æ®
        try:
            daily_data = np.load(self.data_file)  # å½¢çŠ¶åº”ä¸º (144, 4, 500, 900)
            self.num_frames = daily_data.shape[0] - self.input_length
            print(f"å·²åŠ è½½å›æº¯æµ‹è¯•æ•°æ®: {self.data_file}, å¯ç”¨åºåˆ—æ•°: {self.num_frames}")
            
            # åˆ›å»ºNaNæ©ç 
            self.data_mask = np.isnan(daily_data)
            
            # ä½¿ç”¨æ—¶ç©ºæ’å€¼å¡«å……NaN
            self.daily_data = self.interpolate_nans(daily_data)
            
            # è½¬æ¢ä¸ºtorchå¼ é‡
            self.daily_data = torch.from_numpy(self.daily_data).float()
            self.data_mask = torch.from_numpy(self.data_mask).bool()
            
        except Exception as e:
            print(f"åŠ è½½æ–‡ä»¶ {self.data_file} å¤±è´¥: {e}")
            self.daily_data = torch.zeros((144, 4, 500, 900), dtype=torch.float32)
            self.data_mask = torch.ones((144, 4, 500, 900), dtype=torch.bool)  # å…¨éƒ¨æ ‡è®°ä¸ºNaN
            self.num_frames = 0
    
    def interpolate_nans(self, data):
        """
        ä½¿ç”¨æ—¶ç©ºæ’å€¼å¡«å……NaNå€¼
        Args:
            data: åŸå§‹æ•°æ®ï¼Œå½¢çŠ¶ä¸º (T, C, H, W)
        Returns:
            filled_data: å¡«å……åçš„æ•°æ®
        """
        # æ—¶é—´ç»´åº¦æ’å€¼
        for c in range(data.shape[1]):
            for h in range(data.shape[2]):
                for w in range(data.shape[3]):
                    series = data[:, c, h, w]
                    if np.any(np.isnan(series)):
                        valid_indices = np.where(~np.isnan(series))[0]
                        if len(valid_indices) > 0:  # å¦‚æœæœ‰æœ‰æ•ˆå€¼
                            nan_indices = np.where(np.isnan(series))[0]
                            # ä½¿ç”¨æœ‰æ•ˆå€¼è¿›è¡Œæ’å€¼
                            series[nan_indices] = np.interp(
                                nan_indices, valid_indices, series[valid_indices],
                                left=0, right=0  # è¾¹ç•Œå¤–ä½¿ç”¨0
                            )
        
        # ç©ºé—´ç»´åº¦æ’å€¼ - å¯¹äºæ—¶é—´æ’å€¼åä»ç„¶æœ‰NaNçš„ä½ç½®
        for t in range(data.shape[0]):
            for c in range(data.shape[1]):
                frame = data[t, c]
                if np.any(np.isnan(frame)):
                    # è·å–éNaNä½ç½®çš„åæ ‡å’Œå€¼
                    y_indices, x_indices = np.where(~np.isnan(frame))
                    if len(y_indices) > 0:  # å¦‚æœæœ‰æœ‰æ•ˆå€¼
                        points = np.column_stack((y_indices, x_indices))
                        values = frame[~np.isnan(frame)]
                        
                        # è·å–NaNä½ç½®çš„åæ ‡
                        nan_y_indices, nan_x_indices = np.where(np.isnan(frame))
                        nan_points = np.column_stack((nan_y_indices, nan_x_indices))
                        
                        # ä½¿ç”¨æœ€è¿‘é‚»æ’å€¼
                        if len(points) > 3:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç‚¹è¿›è¡Œæ’å€¼
                            try:
                                interpolator = NearestNDInterpolator(points, values)
                                frame[nan_y_indices, nan_x_indices] = interpolator(nan_points)
                            except Exception as e:
                                print(f"ç©ºé—´æ’å€¼å¤±è´¥: {e}")
                                frame[np.isnan(frame)] = 0.0  # å¦‚æœæ’å€¼å¤±è´¥ï¼Œä½¿ç”¨0å¡«å……
                        else:
                            frame[np.isnan(frame)] = 0.0  # å¦‚æœæœ‰æ•ˆç‚¹å¤ªå°‘ï¼Œä½¿ç”¨0å¡«å……
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰NaNï¼Œå¦‚æœæœ‰åˆ™å¡«å……ä¸º0
        if np.any(np.isnan(data)):
            data = np.nan_to_num(data, nan=0.0)
            
        return data
    
    def __len__(self):
        """è¿”å›æ•°æ®é›†å¤§å°"""
        return max(0, self.num_frames)
    
    def __getitem__(self, index):
        """è·å–æŒ‡å®šç´¢å¼•çš„æ ·æœ¬"""
        # æå–è¾“å…¥åºåˆ—åŠå…¶æ©ç 
        input_tensor = self.daily_data[index:index+self.input_length, :3]  # åªå–å‰3ä¸ªé€šé“
        input_mask = self.data_mask[index:index+self.input_length, :3]
        
        return input_tensor, input_mask


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    configs = {
        'data_path': '/scratch/jianhao/fusion',
        'input_length': 12,  # è¾“å…¥åºåˆ—é•¿åº¦
        'output_length': 12,  # è¾“å‡ºåºåˆ—é•¿åº¦
    }
    
    # æµ‹è¯•è®­ç»ƒæ•°æ®é›†
    train_dataset = FusionDataset(configs, 'train')
    print(f"è®­ç»ƒé›†æ ·æœ¬æ•°é‡: {len(train_dataset)}")
    
    # è·å–ä¸€ä¸ªæ ·æœ¬
    input_data, input_mask, target_data, target_mask = train_dataset[0]
    print(f"è¾“å…¥æ•°æ®å½¢çŠ¶: {input_data.shape}")
    print(f"è¾“å…¥æ©ç å½¢çŠ¶: {input_mask.shape}")
    print(f"ç›®æ ‡æ•°æ®å½¢çŠ¶: {target_data.shape}")
    print(f"ç›®æ ‡æ©ç å½¢çŠ¶: {target_mask.shape}")
    
    # æµ‹è¯•å›ç®—æ¨¡å¼æ•°æ®é›†
    hindcast_dataset = FusionDatasetHindcast(configs)
    print(f"å›ç®—æ¨¡å¼æ ·æœ¬æ•°é‡: {len(hindcast_dataset)}")
    
    # è·å–ä¸€ä¸ªæ ·æœ¬
    input_data, input_mask = hindcast_dataset[0]
    print(f"è¾“å…¥æ•°æ®å½¢çŠ¶: {input_data.shape}")
    print(f"è¾“å…¥æ©ç å½¢çŠ¶: {input_mask.shape}") 