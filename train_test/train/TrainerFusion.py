import datetime
import os
import gc
import numpy as np
import torch
import torch.utils.data as Data
from train.FusionDataset import FusionDataset
from torch.optim import AdamW
from tqdm import tqdm
from utils.CustomLoss import weighted_precipitation_loss
from utils.PreProcess import PreProcess
import torch.nn.functional as F


class Trainer(object):
    def __init__(self, configs, model, train_dataset=None, val_dataset=None, clearml_logger=None):
        self.configs = configs
        self.model = model
        self.model = self.model.to(self.configs["device"])  # æ¨¡å‹åŠ è½½åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆå¦‚GPUï¼‰
        self.loss_fn = weighted_precipitation_loss  # ä½¿ç”¨æ”¯æŒæ©ç çš„åŠ æƒé™æ°´æŸå¤±å‡½æ•°

        # åˆå§‹åŒ–è®­ç»ƒå‚æ•°
        self.cur_epoch = 1
        self.best_epoch = 1
        self.best_mse = float('inf')  # è®°å½•æœ€ä½³MSEï¼ˆè¶Šå°è¶Šå¥½ï¼‰

        # è®¾ç½®è¯„ä¼°é˜ˆå€¼ï¼ˆå¯¹åº”0.1mm/h, 1mm/h, 5mm/hï¼‰
        self.thresholds = [0.001, 0.01, 0.05]

        # ä½¿ç”¨ä¼ å…¥çš„ClearMLæ—¥å¿—å™¨
        self.clearml_logger = clearml_logger
        self.use_clearml = clearml_logger is not None

        # å­˜å‚¨æ•°æ®é›†å¼•ç”¨
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset

        if self.configs["is_train"]:
            # å¦‚æœæ˜¯è®­ç»ƒæ¨¡å¼ï¼Œåˆå§‹åŒ–ä¼˜åŒ–å™¨
            self.optimizer = AdamW(self.model.parameters(), lr=configs["learning_rate"], weight_decay=1e-5)

            # ä½¿ç”¨é¢„å…ˆåŠ è½½çš„æ•°æ®é›†åˆ›å»ºæ•°æ®åŠ è½½å™¨
            if train_dataset is not None and val_dataset is not None:
                self.train_data = Data.DataLoader(
                    train_dataset,
                    batch_size=self.configs["batch_size"],
                    shuffle=True,
                    drop_last=True,
                    num_workers=4,  # å¢åŠ æ•°æ®åŠ è½½çº¿ç¨‹
                    pin_memory=True,  # ä½¿ç”¨å›ºå®šå†…å­˜åŠ é€Ÿæ•°æ®ä¼ è¾“
                )
                self.val_data = Data.DataLoader(
                    val_dataset,
                    batch_size=self.configs["batch_size"],
                    shuffle=False,
                    drop_last=True,
                    num_workers=4,  # å¢åŠ æ•°æ®åŠ è½½çº¿ç¨‹
                    pin_memory=True,  # ä½¿ç”¨å›ºå®šå†…å­˜åŠ é€Ÿæ•°æ®ä¼ è¾“
                )
            else:
                # å¦‚æœæœªæä¾›é¢„åŠ è½½çš„æ•°æ®é›†ï¼Œåˆ™åˆ›å»ºæ–°çš„æ•°æ®é›†å®ä¾‹
                print("è­¦å‘Šï¼šæœªæä¾›é¢„åŠ è½½çš„æ•°æ®é›†ï¼Œå°†åˆ›å»ºæ–°çš„æ•°æ®é›†å®ä¾‹")
                self.train_data = Data.DataLoader(
                    FusionDataset(configs, 'train'),
                    batch_size=self.configs["batch_size"],
                    shuffle=True,
                    drop_last=True,
                    num_workers=4,
                    pin_memory=True,
                )
                self.val_data = Data.DataLoader(
                    FusionDataset(configs, 'valid'),
                    batch_size=self.configs["batch_size"],
                    shuffle=False,
                    drop_last=True,
                    num_workers=4,
                    pin_memory=True,
                )
                
            # å­¦ä¹ ç‡è°ƒåº¦å™¨è®¾ç½®ä¸ºOneCycleLR
            self.scheduler = torch.optim.lr_scheduler.OneCycleLR(
                self.optimizer,
                max_lr=configs["learning_rate"],
                epochs=configs["epoch"],
                steps_per_epoch=len(self.train_data),
                pct_start=0.3
            )
        else:
            # å¦‚æœæ˜¯æµ‹è¯•æ¨¡å¼ï¼Œåˆå§‹åŒ–æµ‹è¯•æ•°æ®åŠ è½½å™¨
            if val_dataset is not None:
                # ä¸ºæµ‹è¯•æ¨¡å¼å®šä¹‰è‡ªå®šä¹‰collateå‡½æ•°å¤„ç†æ—¶é—´ä¿¡æ¯
                def custom_collate_fn(batch):
                    if len(batch[0]) == 6:  # æµ‹è¯•æ¨¡å¼æœ‰æ—¶é—´ä¿¡æ¯
                        input_tensors = torch.stack([item[0] for item in batch])
                        input_masks = torch.stack([item[1] for item in batch])
                        target_tensors = torch.stack([item[2] for item in batch])
                        target_masks = torch.stack([item[3] for item in batch])
                        input_times = [item[4] for item in batch]  # ä¿æŒä¸ºåˆ—è¡¨
                        target_times = [item[5] for item in batch]  # ä¿æŒä¸ºåˆ—è¡¨
                        return input_tensors, input_masks, target_tensors, target_masks, input_times, target_times
                    else:  # è®­ç»ƒ/éªŒè¯æ¨¡å¼
                        return torch.utils.data.dataloader.default_collate(batch)
                
                self.val_data = Data.DataLoader(
                    val_dataset,
                    batch_size=self.configs["batch_size"],
                    shuffle=False,
                    drop_last=True,
                    num_workers=0,  # å‡å°‘æ•°æ®åŠ è½½çº¿ç¨‹ï¼Œé¿å…CUDAå†…å­˜é”™è¯¯
                    pin_memory=False,  # ç¦ç”¨å›ºå®šå†…å­˜ï¼Œé¿å…CUDAå†…å­˜é”™è¯¯
                    collate_fn=custom_collate_fn,  # ä½¿ç”¨è‡ªå®šä¹‰collateå‡½æ•°
                )
            else:
                # å¦‚æœæœªæä¾›é¢„åŠ è½½çš„æµ‹è¯•é›†ï¼Œåˆ™åˆ›å»ºæ–°çš„æ•°æ®é›†å®ä¾‹
                print("è­¦å‘Šï¼šæœªæä¾›é¢„åŠ è½½çš„æµ‹è¯•é›†ï¼Œå°†åˆ›å»ºæ–°çš„æ•°æ®é›†å®ä¾‹")
                
                # åŒæ ·éœ€è¦è‡ªå®šä¹‰collateå‡½æ•°
                def custom_collate_fn(batch):
                    if len(batch[0]) == 6:  # æµ‹è¯•æ¨¡å¼æœ‰æ—¶é—´ä¿¡æ¯
                        input_tensors = torch.stack([item[0] for item in batch])
                        input_masks = torch.stack([item[1] for item in batch])
                        target_tensors = torch.stack([item[2] for item in batch])
                        target_masks = torch.stack([item[3] for item in batch])
                        input_times = [item[4] for item in batch]  # ä¿æŒä¸ºåˆ—è¡¨
                        target_times = [item[5] for item in batch]  # ä¿æŒä¸ºåˆ—è¡¨
                        return input_tensors, input_masks, target_tensors, target_masks, input_times, target_times
                    else:  # è®­ç»ƒ/éªŒè¯æ¨¡å¼
                        return torch.utils.data.dataloader.default_collate(batch)
                
                self.val_data = Data.DataLoader(
                    FusionDataset(configs, 'test'),
                    batch_size=self.configs["batch_size"],
                    shuffle=False,
                    drop_last=True,
                    num_workers=0,  # å‡å°‘æ•°æ®åŠ è½½çº¿ç¨‹ï¼Œé¿å…CUDAå†…å­˜é”™è¯¯
                    pin_memory=False,  # ç¦ç”¨å›ºå®šå†…å­˜ï¼Œé¿å…CUDAå†…å­˜é”™è¯¯
                    collate_fn=custom_collate_fn,  # ä½¿ç”¨è‡ªå®šä¹‰collateå‡½æ•°
                )
            
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        self.log_dir = '/home/jianhao/methods/single-source/logs'
        os.makedirs(self.log_dir, exist_ok=True)
        self.log_file = os.path.join(self.log_dir, f'train_log_{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}.txt')
        
        # å¦‚æœä¿å­˜ç›®å½•ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºç›®å½•
        if not os.path.isdir(self.configs["save_dir"]):
            os.makedirs(self.configs["save_dir"])
        # å¦‚æœé¢„æµ‹å›¾ç‰‡å­˜æ”¾è·¯å¾„ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºç›®å½•
        if not os.path.exists(self.configs["pred_img_path"]):
            os.makedirs(self.configs["pred_img_path"])

    def log(self, message):
        """è®°å½•æ—¥å¿—åˆ°æ–‡ä»¶"""
        with open(self.log_file, 'a') as f:
            f.write(f"{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {message}\n")
        print(message)

    def log_to_clearml(self, title, series, value, iteration):
        """è®°å½•æ•°æ®åˆ°ClearMLï¼ˆå¦‚æœå¯ç”¨ï¼‰"""
        if self.use_clearml and self.clearml_logger:
            self.clearml_logger(title, series, value, iteration)
            
    def calculate_metrics(self, output, target, mask=None, threshold=0.001):
        """
        è®¡ç®—ç»™å®šé˜ˆå€¼ä¸‹çš„POD, FAR, CSIæŒ‡æ ‡ï¼Œæ”¯æŒæ©ç 
        Args:
            output: é¢„æµ‹å€¼
            target: ç›®æ ‡å€¼
            mask: æ©ç ï¼ŒTrueè¡¨ç¤ºæ˜¯NaNä½ç½®ï¼ˆéœ€è¦å¿½ç•¥ï¼‰
            threshold: é˜ˆå€¼
        Returns:
            pod, far, csi: è¯„ä¼°æŒ‡æ ‡
        """
        # å¦‚æœæ²¡æœ‰æä¾›æ©ç ï¼Œåˆ›å»ºä¸€ä¸ªå…¨Falseçš„æ©ç ï¼ˆä¸å¿½ç•¥ä»»ä½•ä½ç½®ï¼‰
        if mask is None:
            mask = torch.zeros_like(target, dtype=torch.bool)
        
        # åˆ›å»ºæœ‰æ•ˆä½ç½®çš„æ©ç ï¼ˆéNaNä½ç½®ï¼‰
        valid_mask = ~mask
        
        # å°†é¢„æµ‹å€¼å’ŒçœŸå®å€¼äºŒå€¼åŒ–ï¼Œåªè€ƒè™‘æœ‰æ•ˆä½ç½®
        pred_binary = (output > threshold).float() * valid_mask.float()
        true_binary = (target > threshold).float() * valid_mask.float()
        
        # è®¡ç®—TP, FP, FN
        tp = torch.sum((pred_binary == 1) & (true_binary == 1)).item()
        fp = torch.sum((pred_binary == 1) & (true_binary == 0)).item()
        fn = torch.sum((pred_binary == 0) & (true_binary == 1)).item()
        
        # è®¡ç®—æŒ‡æ ‡
        pod = tp / (tp + fn) if (tp + fn) > 0 else 0
        far = fp / (tp + fp) if (tp + fp) > 0 else 0
        csi = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0
        
        return pod, far, csi

    def train(self):
        """è®­ç»ƒæ¨¡å‹"""
        self.log(f"å¼€å§‹è®­ç»ƒï¼Œæ€»å…± {self.configs['epoch']} ä¸ªepoch")

        # ç›´æ¥è®­ç»ƒæ‰€æœ‰epoch
        self.train_all_epochs()

        self.log("è®­ç»ƒå®Œæˆï¼")



    def train_all_epochs(self):
        """è®­ç»ƒæ‰€æœ‰epoch"""
        # åªåœ¨å¼€å§‹æ—¶è¾“å‡ºä¸€æ¬¡æ ·æœ¬ç»Ÿè®¡
        if not hasattr(self, '_logged_sample_info'):
            print(f"ä½¿ç”¨ä¸€æ¬¡æ€§è®­ç»ƒç­–ç•¥:")
            print(f"   è®­ç»ƒé›†æ ·æœ¬æ•°: {len(self.train_dataset)}")
            print(f"   è®­ç»ƒé›†æ‰¹æ¬¡æ•°: {len(self.train_data)}")
            print(f"ğŸš€ å¼€å§‹è®­ç»ƒ...")
            self._logged_sample_info = True

        train_loss = []  # è®­ç»ƒæŸå¤±è®°å½•
        global_step = getattr(self, 'global_step', 0)  # å…¨å±€æ­¥æ•°ï¼Œç”¨äºClearMLè®°å½•
        for epoch in range(self.cur_epoch, self.configs["epoch"] + 1):
            t = tqdm(
                self.train_data, desc="è®­ç»ƒä¸­", leave=True, total=len(self.train_data)
            )
            epoch_loss = []
            epoch_mse = []
            
            # ç”¨äºè®¡ç®—æ¯ä¸ªepochçš„è¯„ä¼°æŒ‡æ ‡
            metrics_data = {th: {'tp': 0, 'fp': 0, 'fn': 0} for th in self.thresholds}
            
            for batch in t:
                # è§£åŒ…æ‰¹æ¬¡æ•°æ®ï¼ˆç°åœ¨åŒ…å«æ©ç ï¼‰
                input_ims, input_masks, label_ims, label_masks = batch
                
                self.model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
                # æ•°æ®é¢„å¤„ç†ï¼šreshape
                input_ims = PreProcess.reshape_patch(
                    input_ims, self.configs["patch_size"]
                )
                input_ims = input_ims.to(self.configs["device"])   # ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
                label_ims = label_ims.to(self.configs["device"])   # ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
                
                # å°†æ©ç ä¹Ÿç§»åŠ¨åˆ°è®¾å¤‡å¹¶reshape
                input_masks = PreProcess.reshape_patch(
                    input_masks.float(), self.configs["patch_size"]
                ).bool()
                input_masks = input_masks.to(self.configs["device"])
                label_masks = label_masks.to(self.configs["device"])
                
                # æ¨¡å‹å‰å‘ä¼ æ’­
                output_ims = self.model(input_ims)
                output_ims = PreProcess.reshape_patch_back(
                    output_ims, self.configs["patch_size"]
                )  # åå‘reshape

                self.optimizer.zero_grad()  # æ¸…é›¶æ¢¯åº¦
                # è®¡ç®—æŸå¤±ï¼ˆè€ƒè™‘æ©ç ï¼‰
                loss = F.mse_loss(output_ims, label_ims, reduction='mean')  # é»˜è®¤å°±æ˜¯ mean
                
                loss.backward()  # åå‘ä¼ æ’­
                self.optimizer.step()  # æ›´æ–°å‚æ•°
                self.scheduler.step()  # æ›´æ–°å­¦ä¹ ç‡

                # è®¡ç®—MSEï¼ˆä¸å¸¦æƒé‡ï¼Œä½†è€ƒè™‘æ©ç ï¼‰
                valid_mask = ~label_masks
                if valid_mask.sum() > 0:
                    mse = torch.sum(((output_ims - label_ims) ** 2) * valid_mask.float()) / valid_mask.sum()
                else:
                    mse = torch.tensor(0.0, device=output_ims.device)
                

                # è®°å½•åˆ°ClearML - æ¯ä¸ªæŒ‡æ ‡å•ç‹¬è®°å½•
                # self.log_to_clearml("Loss", "Train", loss.item(), global_step)
                # self.log_to_clearml("MSE", "Train", mse.item(), global_step)
                # self.log_to_clearml("Learning Rate", "Train", self.optimizer.param_groups[0]['lr'], global_step)
                
                # ç´¯è®¡æ¯ä¸ªbatchçš„è¯„ä¼°æŒ‡æ ‡æ•°æ®

                
                global_step += 1
                epoch_loss.append(loss.detach().cpu().numpy().item())  # è®°å½•æŸå¤±
                epoch_mse.append(mse.detach().cpu().numpy().item())    # è®°å½•MSE
                t.set_postfix(
                    {
                        "loss": "{:.6f}".format(loss.detach().cpu().numpy().item()),
                        "mse": "{:.6f}".format(mse.detach().cpu().numpy().item()),
                        "epoch": "{}".format(epoch),
                    }
                )
            
            # è®¡ç®—å¹¶è®°å½•å¹³å‡è®­ç»ƒæŸå¤±å’ŒMSE
            avg_train_loss = np.mean(epoch_loss)
            avg_train_mse = np.mean(epoch_mse)
            train_loss.append(avg_train_loss)
            self.log(f"Epoch {epoch}/{self.configs['epoch']} - å¹³å‡è®­ç»ƒæŸå¤±: {avg_train_loss:.6f}, MSE: {avg_train_mse:.6f}")
            
            # æ¯ä¸ªepochè¿›è¡ŒéªŒè¯
            avg_mse, val_metrics = self.validate(epoch)
            
            # è®°å½•åˆ°ClearML - Epochçº§åˆ«æŒ‡æ ‡
            self.log_to_clearml("Epoch Loss", "Train", avg_train_loss, epoch)
            self.log_to_clearml("Epoch MSE", "Train", avg_train_mse, epoch)
            self.log_to_clearml("Epoch MSE", "Validation", avg_mse, epoch)
            self.log_to_clearml("Learning Rate", "Train", self.optimizer.param_groups[0]['lr'], epoch)
            
            # ä¿å­˜æœ€ä¼˜æ¨¡å‹
            if avg_mse < self.best_mse:
                self.best_mse = avg_mse
                self.best_epoch = epoch
                self.save(epoch, is_best=True)
                self.log(f"æ–°çš„æœ€ä½³æ¨¡å‹! Epoch {epoch} - MSE: {avg_mse:.6f}")
            
            
            self.log(f"å½“å‰æœ€ä½³æ¨¡å‹: Epoch {self.best_epoch} - MSE: {self.best_mse:.6f}")

        # æ›´æ–°å½“å‰epoch
        self.cur_epoch = self.configs["epoch"] + 1
        self.global_step = global_step  # ä¿å­˜å…¨å±€æ­¥æ•°

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.save(self.configs["epoch"], is_best=False, is_final=True)
        self.log(f"è®­ç»ƒå®Œæˆ! æœ€ä½³æ¨¡å‹: Epoch {self.best_epoch} - MSE: {self.best_mse:.6f}")

    def validate(self, epoch):
        """éªŒè¯æ¨¡å‹æ€§èƒ½"""
        self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        valid_loss = []
        mse_values = []
        
        # ç”¨äºè®¡ç®—è¯„ä¼°æŒ‡æ ‡
        metrics_data = {th: {'pod': [], 'far': [], 'csi': []} for th in self.thresholds}
        
        with torch.no_grad():
            for batch in tqdm(self.val_data, desc="éªŒè¯ä¸­", leave=False):
                # è§£åŒ…æ‰¹æ¬¡æ•°æ®ï¼ˆç°åœ¨åŒ…å«æ©ç ï¼‰
                input_ims, input_masks, label_ims, label_masks = batch
                
                # æ•°æ®é¢„å¤„ç†ï¼šreshape
                input_ims = PreProcess.reshape_patch(
                    input_ims, self.configs["patch_size"]
                )
                input_ims = input_ims.to(self.configs["device"])
                label_ims = label_ims.to(self.configs["device"])
                
                # å°†æ©ç ä¹Ÿç§»åŠ¨åˆ°è®¾å¤‡å¹¶reshape
                input_masks = PreProcess.reshape_patch(
                    input_masks.float(), self.configs["patch_size"]
                ).bool()
                input_masks = input_masks.to(self.configs["device"])
                label_masks = label_masks.to(self.configs["device"])
                
                # æ¨¡å‹å‰å‘ä¼ æ’­
                output_ims = self.model(input_ims)
                output_ims = PreProcess.reshape_patch_back(
                    output_ims, self.configs["patch_size"]
                )
                
                # è®¡ç®—æŸå¤±ï¼ˆè€ƒè™‘æ©ç ï¼‰
                loss = self.loss_fn(output_ims, label_ims, label_masks)
                valid_loss.append(loss.item())
                
                # è®¡ç®—MSEï¼ˆä¸å¸¦æƒé‡ï¼Œä½†è€ƒè™‘æ©ç ï¼‰
                valid_mask = ~label_masks
                if valid_mask.sum() > 0:
                    mse = torch.sum(((output_ims - label_ims) ** 2) * valid_mask.float()) / valid_mask.sum()
                else:
                    mse = torch.tensor(0.0, device=output_ims.device)
                mse_values.append(mse.item())
                
                # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
                for th in self.thresholds:
                    pod, far, csi = self.calculate_metrics(output_ims, label_ims, label_masks, th)
                    metrics_data[th]['pod'].append(pod)
                    metrics_data[th]['far'].append(far)
                    metrics_data[th]['csi'].append(csi)
        
        # è®¡ç®—å¹³å‡æŸå¤±å’ŒMSE
        avg_valid_loss = np.mean(valid_loss)
        avg_mse = np.mean(mse_values)
        
        # è®¡ç®—å¹¶è®°å½•å¹³å‡è¯„ä¼°æŒ‡æ ‡
        val_metrics = {}
        for th in self.thresholds:
            avg_pod = np.mean(metrics_data[th]['pod'])
            avg_far = np.mean(metrics_data[th]['far'])
            avg_csi = np.mean(metrics_data[th]['csi'])
            
            val_metrics[th] = {
                'pod': avg_pod,
                'far': avg_far,
                'csi': avg_csi
            }
            
            threshold_str = f"{th*100:.1f}mm"
            self.log(f"éªŒè¯æŒ‡æ ‡ (é˜ˆå€¼ {threshold_str}/h) - POD: {avg_pod:.4f}, FAR: {avg_far:.4f}, CSI: {avg_csi:.4f}")
            
            # è®°å½•åˆ°ClearML
            self.log_to_clearml(f"POD", f"Validation {threshold_str}", avg_pod, epoch)
            self.log_to_clearml(f"FAR", f"Validation {threshold_str}", avg_far, epoch)
            self.log_to_clearml(f"CSI", f"Validation {threshold_str}", avg_csi, epoch)
        
        self.log(f"éªŒè¯ - å¹³å‡æŸå¤±: {avg_valid_loss:.6f}, MSE: {avg_mse:.6f}")
        return avg_mse, val_metrics

    def test(self):
        """æµ‹è¯•æ¨¡å‹æ€§èƒ½"""
        self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        test_loss = []
        mse_values = []
        
        # ç”¨äºè®¡ç®—è¯„ä¼°æŒ‡æ ‡
        metrics_data = {th: {'pod': [], 'far': [], 'csi': []} for th in self.thresholds}
        
        # ä¸ºä¿å­˜é¢„æµ‹ç»“æœåˆ›å»ºç›®å½•
        pred_dir = self.configs["pred_img_path"]
        os.makedirs(pred_dir, exist_ok=True)
        
        # æ ·æœ¬è®¡æ•°å™¨
        sample_count = 0
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(tqdm(self.val_data, desc="æµ‹è¯•ä¸­", leave=False)):
                # è§£åŒ…æ‰¹æ¬¡æ•°æ®ï¼ˆæµ‹è¯•æ¨¡å¼åŒ…å«æ—¶é—´ä¿¡æ¯ï¼‰
                if len(batch) == 6:  # æµ‹è¯•æ¨¡å¼è¿”å›6ä¸ªå…ƒç´ 
                    input_ims, input_masks, label_ims, label_masks, input_times, target_times = batch
                else:  # è®­ç»ƒ/éªŒè¯æ¨¡å¼è¿”å›4ä¸ªå…ƒç´ 
                    input_ims, input_masks, label_ims, label_masks = batch
                    input_times, target_times = None, None
                
                # æ•°æ®é¢„å¤„ç†ï¼šreshape
                input_ims = PreProcess.reshape_patch(
                    input_ims, self.configs["patch_size"]
                )
                input_ims = input_ims.to(self.configs["device"])
                label_ims = label_ims.to(self.configs["device"])
                
                # å°†æ©ç ä¹Ÿç§»åŠ¨åˆ°è®¾å¤‡å¹¶reshape
                input_masks = PreProcess.reshape_patch(
                    input_masks.float(), self.configs["patch_size"]
                ).bool()
                input_masks = input_masks.to(self.configs["device"])
                label_masks = label_masks.to(self.configs["device"])
                
                # æ¨¡å‹å‰å‘ä¼ æ’­
                output_ims = self.model(input_ims)
                output_ims = PreProcess.reshape_patch_back(
                    output_ims, self.configs["patch_size"]
                )
                
                # è®¡ç®—æŸå¤±ï¼ˆè€ƒè™‘æ©ç ï¼‰
                loss = self.loss_fn(output_ims, label_ims, label_masks)
                test_loss.append(loss.item())
                
                # è®¡ç®—MSEï¼ˆä¸å¸¦æƒé‡ï¼Œä½†è€ƒè™‘æ©ç ï¼‰
                valid_mask = ~label_masks
                if valid_mask.sum() > 0:
                    mse = torch.sum(((output_ims - label_ims) ** 2) * valid_mask.float()) / valid_mask.sum()
                else:
                    mse = torch.tensor(0.0, device=output_ims.device)
                mse_values.append(mse.item())
                
                # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
                for th in self.thresholds:
                    pod, far, csi = self.calculate_metrics(output_ims, label_ims, label_masks, th)
                    metrics_data[th]['pod'].append(pod)
                    metrics_data[th]['far'].append(far)
                    metrics_data[th]['csi'].append(csi)
                
                # ä¿å­˜æ‰€æœ‰é¢„æµ‹ç»“æœ
                    # ä¸ºå½“å‰æ‰¹æ¬¡åˆ›å»ºç›®å½•
                    batch_dir = os.path.join(pred_dir, f"batch_{batch_idx}")
                    os.makedirs(batch_dir, exist_ok=True)
                    
                    # æ¯ä¸ªæ ·æœ¬åœ¨æ‰¹æ¬¡ä¸­ä¿å­˜
                    for i in range(input_ims.shape[0]):
                        # åˆ›å»ºæ ·æœ¬ç›®å½•
                        sample_dir = os.path.join(batch_dir, f"sample_{sample_count}")
                        os.makedirs(sample_dir, exist_ok=True)
                        
                        # å°†é¢„æµ‹ã€çœŸå®å€¼å’Œè¾“å…¥ç§»åˆ°CPUå¹¶è½¬ä¸ºNumPyæ•°ç»„
                        output_sample = output_ims[i].detach().cpu().numpy()
                        label_sample = label_ims[i].detach().cpu().numpy()
                        
                    # å¦‚æœæœ‰æ—¶é—´ä¿¡æ¯ï¼Œä¿å­˜æ—¶é—´ä¿¡æ¯
                    if target_times is not None:
                        # ä¿å­˜æ—¶é—´ä¿¡æ¯åˆ°æ–‡ä»¶
                        time_info = {
                            'input_times': input_times[i] if input_times else [],
                            'target_times': target_times[i] if target_times else []
                        }
                        import json
                        with open(os.path.join(sample_dir, "time_info.json"), 'w') as f:
                            json.dump(time_info, f, indent=2)
                    
                        # ä¿å­˜æ¯ä¸ªæ—¶é—´æ­¥çš„é¢„æµ‹å’ŒçœŸå®å€¼
                        for t in range(output_sample.shape[0]):
                            pred_file = os.path.join(sample_dir, f"pred_{t}.npy")
                            gt_file = os.path.join(sample_dir, f"gt_{t}.npy")
                            
                            # ä¿å­˜ä¸ºNumPyæ–‡ä»¶
                            np.save(pred_file, output_sample[t, 0])  # åªä¿å­˜ç¬¬ä¸€ä¸ªé€šé“
                            np.save(gt_file, label_sample[t, 0])  # åªä¿å­˜ç¬¬ä¸€ä¸ªé€šé“
                        
                        sample_count += 1
                    
                # æ¯å¤„ç†å®Œä¸€ä¸ªæ‰¹æ¬¡è¾“å‡ºä¸€æ¬¡è¿›åº¦
                if batch_idx % 10 == 0:
                    self.log(f"å·²å¤„ç† {batch_idx + 1} ä¸ªæ‰¹æ¬¡ï¼Œä¿å­˜äº† {sample_count} ä¸ªæ ·æœ¬")
        
        # è®¡ç®—å¹³å‡æŸå¤±å’ŒMSE
        avg_test_loss = np.mean(test_loss)
        avg_mse = np.mean(mse_values)
        
        # è®¡ç®—å¹¶è®°å½•å¹³å‡è¯„ä¼°æŒ‡æ ‡
        self.log(f"æµ‹è¯•ç»“æœ - å¹³å‡æŸå¤±: {avg_test_loss:.6f}, MSE: {avg_mse:.6f}")
        for th in self.thresholds:
            avg_pod = np.mean(metrics_data[th]['pod'])
            avg_far = np.mean(metrics_data[th]['far'])
            avg_csi = np.mean(metrics_data[th]['csi'])
            
            threshold_str = f"{th*100:.1f}mm"
            self.log(f"æµ‹è¯•æŒ‡æ ‡ (é˜ˆå€¼ {threshold_str}/h) - POD: {avg_pod:.4f}, FAR: {avg_far:.4f}, CSI: {avg_csi:.4f}")
        
        self.log(f"é¢„æµ‹ç»“æœä¿å­˜åœ¨: {pred_dir}")
        self.log(f"æ€»å…±ä¿å­˜äº† {sample_count} ä¸ªæµ‹è¯•æ ·æœ¬")
        return avg_mse

    def save(self, epoch, is_best=False, is_final=False):
        """ä¿å­˜æ¨¡å‹"""
        if is_best:
            save_path = os.path.join(self.configs["save_dir"], "best_model.ckpt")
            torch.save(
                {
                    "epoch": epoch,
                    "model_state_dict": self.model.state_dict(),
                    # "optimizer_state_dict": self.optimizer.state_dict(),
                    "best_mse": self.best_mse,
                },
                save_path,
            )
            self.log(f"ä¿å­˜æœ€ä½³æ¨¡å‹åˆ° {save_path}")
        elif is_final:
            save_path = os.path.join(self.configs["save_dir"], "final_model.ckpt")
            torch.save(
                {
                    "epoch": epoch,
                    "model_state_dict": self.model.state_dict(),
                    # "optimizer_state_dict": self.optimizer.state_dict(),
                    "best_mse": self.best_mse,
                },
                save_path,
            )
            self.log(f"ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ° {save_path}")
        else:
            save_path = os.path.join(self.configs["save_dir"], f"model_epoch_{epoch}.ckpt")
            torch.save(
                {
                    "epoch": epoch,
                    "model_state_dict": self.model.state_dict(),
                    # "optimizer_state_dict": self.optimizer.state_dict(),
                    "best_mse": self.best_mse,
                },
                save_path,
            )
            self.log(f"ä¿å­˜æ£€æŸ¥ç‚¹åˆ° {save_path}")

    def load(self, checkpoint_path):
        """åŠ è½½æ¨¡å‹"""
        if not os.path.exists(checkpoint_path):
            self.log(f"è­¦å‘Šï¼šæ£€æŸ¥ç‚¹ {checkpoint_path} ä¸å­˜åœ¨ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒ")
            return
        
        checkpoint = torch.load(checkpoint_path, map_location=self.configs["device"])
        
        # æ£€æŸ¥ä½¿ç”¨å“ªä¸ªé”®æ¥åŠ è½½æ¨¡å‹å‚æ•°
        if "model_state_dict" in checkpoint:
            self.model.load_state_dict(checkpoint["model_state_dict"])
        elif "net_param" in checkpoint:
            self.model.load_state_dict(checkpoint["net_param"])
        else:
            self.log(f"é”™è¯¯ï¼šæ£€æŸ¥ç‚¹æ–‡ä»¶ä¸­æ‰¾ä¸åˆ°æ¨¡å‹å‚æ•°ï¼Œå¯ç”¨çš„é”®: {checkpoint.keys()}")
            return
        
        if self.configs["is_train"]:
            # if "optimizer_state_dict" in checkpoint:
            #     self.optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
            self.cur_epoch = checkpoint.get("epoch", 0) + 1
            self.best_mse = checkpoint.get("best_mse", float('inf'))
            self.log(f"åŠ è½½æ£€æŸ¥ç‚¹ {checkpoint_path}, ä»epoch {self.cur_epoch} å¼€å§‹è®­ç»ƒ, æœ€ä½³MSE: {self.best_mse:.6f}")
        else:
            self.log(f"åŠ è½½æ£€æŸ¥ç‚¹ {checkpoint_path} ç”¨äºæµ‹è¯•, æ¨¡å‹epoch: {checkpoint.get('epoch', 0)}") 